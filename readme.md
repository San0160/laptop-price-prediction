# Laptop Price Prediction ðŸ’»ðŸ“Š

## Project Overview
This project builds an end-to-end **machine learning pipeline** to predict laptop prices based on hardware specifications such as RAM, CPU, GPU, display, and storage.

The focus of this project is not only model accuracy, but also **clean modular code, reproducibility, and industry-style workflow**.

---

## Problem Statement
Laptop prices depend on multiple interacting factors (RAM, processor tier, display quality, storage, etc.).  
The goal is to **predict laptop price (in Euros)** using structured product specifications.

---

## Dataset
- Source: Laptop specifications dataset
- Target variable: `price_euro`
- Data includes:
  - Brand & product details
  - CPU / GPU information
  - RAM, storage, weight
  - Screen size & resolution

---

## Project Structure
laptop-price-prediction/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ raw/
â”‚ â””â”€â”€ laptop_price.csv
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data_cleaning.py
â”‚ â”œâ”€â”€ feature_engineering.py
â”‚ â”œâ”€â”€ encoding.py
â”‚ â”œâ”€â”€ model_training.py
â”‚ â”œâ”€â”€ model_evaluation.py
â”‚ â””â”€â”€ model_persistence.py
â”‚
â”œâ”€â”€ models/ # ignored by git (trained artifacts)
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


---

## ML Pipeline
1. **Data Cleaning**
   - Column standardization
   - Missing value handling
   - Type conversions
   - Resolution parsing (width Ã— height)

2. **Feature Engineering**
   - PPI (Pixels Per Inch)
   - CPU tier simplification
   - GPU type (integrated vs dedicated)
   - Storage flags (SSD / HDD)

3. **Encoding & Data Preparation**
   - One-hot encoding for categorical features
   - Train-test split (80/20)
   - Feature scaling (StandardScaler)

4. **Model Training**
   - Linear Regression (baseline)
   - Ridge Regression (regularized)
   - Random Forest Regressor (final model)

5. **Model Evaluation**
   - MAE
   - RMSE
   - RÂ² score

---

## Model Performance

| Model                  | MAE â†“ | RMSE â†“ | RÂ² â†‘ |
|------------------------|------|--------|------|
| Linear Regression      | ~288 | ~394   | ~0.69 |
| Ridge Regression       | ~288 | ~394   | ~0.69 |
| Random Forest Regressor| ~229 | ~343   | **~0.76** |

âœ… **Random Forest performed best**, capturing non-linear relationships and feature interactions.

---

## Feature Importance (Random Forest)
Top contributors to laptop price:
1. RAM (GB)
2. Weight (kg)
3. Display PPI
4. CPU Tier (i7 / i5)
5. Screen Size

This aligns well with real-world pricing logic.

---

## Reproducibility
- Trained model artifacts are **not stored in version control**
- Models can be regenerated by running:
```bash
python main.py
Tech Stack
Python

Pandas, NumPy

Scikit-learn

Joblib

Git & GitHub

Key Learnings
Importance of a strong baseline

Effect of feature engineering on model performance

Why tree-based models outperform linear models for non-linear problems

Industry-standard ML project structuring

Future Improvements
Hyperparameter tuning (GridSearch / RandomSearch)

Gradient Boosting / XGBoost

Residual analysis & error diagnostics

Model deployment (API)

Author
Aman Deep
Machine Learning Enthusiast | End-to-End ML Pipelines